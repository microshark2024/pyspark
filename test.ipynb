{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6507d3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!cat .devcontainer/devcontainer.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1047671b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!cat .devcontainer/docker-compose.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c25ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' �����ڲ����ⲿ���Ҳ���ǿ����еĳ���\n",
      "���������ļ���\n"
     ]
    }
   ],
   "source": [
    "!cat .devcontainer/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a646298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Python executable 'python3', defaulting to 'C:\\Users\\21037\\AppData\\Local\\Programs\\Python\\Python313\\Scripts\\..' for SPARK_HOME environment variable. Please install Python or specify the correct Python executable in PYSPARK_DRIVER_PYTHON or PYSPARK_PYTHON environment variable to detect SPARK_HOME safely.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ϵͳ�Ҳ���ָ����·����\n",
      "ϵͳ�Ҳ���ָ����·����\n"
     ]
    }
   ],
   "source": [
    "!pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e797eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# 创建了一个 SparkSession 对象。SparkSession 是与 Spark 进行交互的入口\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "# SparkContext 是 Spark 的核心，用于连接到 Spark 集群并负责与集群管理器进行通信，可以用来执行分布式任务\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从指定的文件创建一个 RDD。\n",
    "# PySpark 假设我们引用的是 HDFS 上的文件。\n",
    "data = sc.textFile('demo_file.txt')\n",
    "print(type(data))\n",
    "# collect() 会将 RDD 中的元素收集到一个列表中。\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0af61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark 会跟踪原始数据所在的位置。\n",
    "# MapPartitionsRDD 类似于我们创建的所有 RDD 的一个数组\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a947cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple MapReduce task: Summations\n",
    "data = sc.textFile('number.txt')\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在这里使用strip()是多余的, 这只是为了展示一个例子\n",
    "intdata = data.map(lambda n: int(n))\n",
    "intdata.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889913e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "intdata.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.map()\n",
    "def polynomialk(x):\n",
    "return x**2 + 1\n",
    "data = sc.textFile('number.txt')\n",
    "data.collect()\n",
    "doubles = data.map(lambda n: int(n)).map(lambda n: 2*n)\n",
    "doubles.collect()\n",
    "data.map(lambda n: int(n)).map(polynomialk).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de213440",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RDD.filter()\n",
    "data = sc.textFile('number.txt').map(lambda n: int(n))\n",
    "evens = data.filter(lambda n: n%2==0)\n",
    "print(evens.collect())\n",
    "odds = data.filter(lambda n: n%2!=0)\n",
    "print(odds.collect())\n",
    "sc.addPyFile('prime.py')\n",
    "from prime import is_prime\n",
    "# filter()接受一个布尔函数作为参数，只保留评估为True的元素。\n",
    "primes = data.filter(is_prime)\n",
    "print(primes.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.sample()\n",
    "# sample(withReplacement, fraction, [seed])\n",
    "# RDD.sample()主要用于在数据的一小部分上进行测试。\n",
    "data = sc.textFile('number.txt').map(lambda n: int(n))\n",
    "samp = data.sample(False, 0.5)\n",
    "print(samp.collect())\n",
    "samp = data.sample(True, 0.5)\n",
    "print(samp.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e593579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果我的RDD元素比数字更复杂怎么办？\n",
    "## 类似数据库的文件\n",
    "data = sc.textFile('scientists.txt')\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9318bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最初读取时，每一行都是RDD中的一个单独元素。\n",
    "# 在每个元素上分割空格后，我们得到了我们想要的——每个元素都是一个字符串元组。\n",
    "# 注意：RDD.collect()返回一个列表，但在RDD内部，元素是元组，而不是列表。\n",
    "data = data.map(lambda line: line.split())\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RDD.distinct()\n",
    "data = sc.textFile('scientists.txt')\n",
    "data = data.map(lambda line: line.split())\n",
    "fields = data.map(lambda t: t[3]).distinct()\n",
    "fields.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b056cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RDD.flatMap()\n",
    "data = sc.textFile('numbers_weird.txt')\n",
    "data.collect()\n",
    "#同一个数字列表，但现在它们不是每行一个了...\n",
    "#来自PySpark文档：flatMap(func)类似于map，但是每个输入项可以映射到0个或更多的输出项（所以func应该返回一个序列而不是单个项）。 https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RDD.flatMap()\n",
    "# 所以我们可以认为flatMap()为每个RDD元素产生一个列表，然后将这些列表连接起来。但至关重要的是，输出是另一个RDD，而不是列表。这种操作称为展平，它是函数式编程中的一个常见模式。\n",
    "flattened = data.flatMap(lambda line: [x for x in line.split()])\n",
    "flattened.collect()\n",
    "flattened.map(lambda n: int(n)).reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebc6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.count()\n",
    "data = sc.textFile('demo_file.txt')\n",
    "data = data.flatMap(lambda line: line.split())\n",
    "data = data.map(lambda w: w.lower())\n",
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64744251",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqwords = data.distinct()\n",
    "uniqwords.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38969e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.countByKey()\n",
    "# 注意：在上面的例子中，每个单词都有一个键0，但请注意，在countByKey产生的字典中，值对应于该键出现了多少次。这是因为countByKey统计每个键出现的次数并忽略它们的值。\n",
    "data = sc.textFile('demo_file.txt')\n",
    "data = data.flatMap(lambda line: line.split())\n",
    "data = data.map(lambda w: (w.lower(), 0))\n",
    "data.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8f594",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ps_wordcount.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca88cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n",
    "!spark-submit ps_wordcount.py demo_file.txt wc_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat wc_demo/*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
